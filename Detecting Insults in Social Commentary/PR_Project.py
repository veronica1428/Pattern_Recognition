#! /usr/bin/env python3import csvimport nltkimport reimport stringimport nltk.tagfrom nltk.corpus import stopwordsfrom nltk.stem import WordNetLemmatizerfrom nltk import word_tokenize, pos_tagfreqDict = {}sentID_Dict = {}stop = stopwords.words('english')#stop.extend(string.punctuation)#modified on : 28th march 2015#copying the training set into a new setdef trainSet(filePath):        localList = []    refined_train = []    lineCount = 0        train_open= open(filePath)    csv_train= csv.reader(train_open)        for row in csv_train:        if lineCount == 0:            lineCount = lineCount + 1            continue            localList = [row[0], row[2]]        refined_train.append(localList)        lineCount = lineCount + 1    return refined_train#function to remove the URLsdef remove_url_html(s):    s = ''.join(re.sub("<.*?>", "", s))    s= re.sub(r"(?:\@|https?\://)\S+", "", s)    return(s)#function to replace special values in the string by a spacedef replaceValues(s):    s= s.replace("\\n", " ")    s= s.replace("\\xc2", " ")    s= s.replace("\\xa0", " ")    s= s.encode('ascii', 'ignore').decode('utf-8')    return(s)#function to expand the short formsdef expand_Short_Forms(s):    mapping_open= open('Expansion.csv')    csv_mapping= csv.reader(mapping_open)    result={}    for row in csv_mapping:        key= row[0]        result[key] = row[1]    pattern= re.compile(r'\b(' + '|'. join(result.keys()) + r')\b')    s= pattern.sub(lambda x: result[x.group()], s)    return(s)#to determine whether the word is stopword in english dictionary or notdef remove_stopWord(word):    word = ' '.join([token for token in nltk.word_tokenize(word) if token not in stop])    return word#function to convert data set into lower casedef convert_to_LowerCase(s):    s= s.lower()    return(s)#Date : 28th march 2015#Module: function to remove punctuations from a stringdef remove_punctuation(string):        # define punctuation    punctuations = '''!()-=[]{};:\'"\,<>./?@#$%^&*_~'''        no_punct = ""        for char in string:        if (char in punctuations) or (char.isdigit()):            no_punct = no_punct + ' '        else:            no_punct = no_punct + char    return (no_punct)#Date : 20th april'15#Module: function to perform lemmatizationdef lemmatize_word(string):    wnl = WordNetLemmatizer()    wordnet_tag ={'NN':'n','JJ':'a','VB':'v','RB':'r'}    count = 0    lemmatizedString = ""    lemmaPos = []        ## POS tagging    tagged = nltk.pos_tag(word_tokenize(string))        for t in tagged:        try:            lemmatizedWord = wnl.lemmatize(t[0],wordnet_tag.get(t[1][:2]))                except KeyError:            lemmatizedWord = wnl.lemmatize(t[0])                lemmatizedString = lemmatizedString + ' ' + lemmatizedWord        count = count + 1        return lemmatizedString#function to pre process the datadef pre_process(refined_train, filePath):        print('Pre Processing starts here')    outFile = open(filePath, "w")        for row in refined_train:        #****************************************PRE PROCESSING START HERE**************************************        #Convert to lower case        row[1] = convert_to_LowerCase(row[1])                #Expanding short forms in the training set        row[1] = expand_Short_Forms(row[1])            #Removing Encoding from the training set        row[1] = replaceValues(row[1])            #Removing HTML tags with an empty string        row[1] = remove_url_html(row[1])            #Removing punctuations_Digits from the training set        row[1] = remove_punctuation(row[1])                #Removing stopwords from the training set        row[1] = remove_stopWord(row[1])                #Perform Lemmatization        row[1] = lemmatize_word(row[1])                #write pre processed data to a file        if row[1]:            outFile.write(str(row[0]) + ',' + str(row[1]) + '\n')        else:            continue        #****************************************PRE PROCESSING ENDS HERE**************************************    outFile.close()#function to run main codedef run():    #copy entire training set into a list    trainList = trainSet('train.csv')    #Pre Process the train data    pre_process(trainList, 'trainOut.csv')    #copy entire test set into a list the test data    testList = trainSet('test.csv')    #PreProcess the Test data    pre_process(testList, 'testOut.csv')#function to control codedef main():    print('************************INSIDE MAIN*************************')    run()#Execution will start from heremain()